{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39cb5563",
   "metadata": {},
   "source": [
    "## Question 1. Install Spark and PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f01888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb7b1410",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/canovasjm/spark/spark-3.0.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.0.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/02/27 20:26:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('homework') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f31716e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29ca0c2",
   "metadata": {},
   "source": [
    "## Question 2. HVFHW February 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8971fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44224cf8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-02-23 16:23:58--  https://nyc-tlc.s3.amazonaws.com/trip+data/fhvhv_tripdata_2021-02.csv\n",
      "Resolving nyc-tlc.s3.amazonaws.com (nyc-tlc.s3.amazonaws.com)... 52.217.43.60\n",
      "Connecting to nyc-tlc.s3.amazonaws.com (nyc-tlc.s3.amazonaws.com)|52.217.43.60|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 733822658 (700M) [text/csv]\n",
      "Saving to: ‘fhvhv_tripdata_2021-02.csv’\n",
      "\n",
      "fhvhv_tripdata_2021 100%[===================>] 699.83M  37.0MB/s    in 14s     \n",
      "\n",
      "2022-02-23 16:24:13 (49.0 MB/s) - ‘fhvhv_tripdata_2021-02.csv’ saved [733822658/733822658]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download the file\n",
    "!wget https://nyc-tlc.s3.amazonaws.com/trip+data/fhvhv_tripdata_2021-02.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8c14320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the schema\n",
    "schema = types.StructType([\n",
    "    types.StructField('hvfhs_license_num', types.StringType(), True),\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa9ae64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the file\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('fhvhv_tripdata_2021-02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a487580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform repartition\n",
    "df = df.repartition(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f7c2a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# save parquet file\n",
    "df.write.parquet('fhvhv/2021/02/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36064cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210M\tfhvhv/2021/02/\r\n"
     ]
    }
   ],
   "source": [
    "# check the size of folder fhvhv/2021/02/\n",
    "#!ls fhvhv/2021/02/ -lh\n",
    "!du -sh fhvhv/2021/02/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f391f9",
   "metadata": {},
   "source": [
    "## Question 3. Count records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0df53797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11613943 fhvhv_tripdata_2021-02.csv\r\n"
     ]
    }
   ],
   "source": [
    "# get the total number of rows in February file\n",
    "!wc -l fhvhv_tripdata_2021-02.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc550485",
   "metadata": {},
   "source": [
    "### Using pyspark data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c427ae61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19ff66fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 2 new columns: pickup_date and dropoff_date\n",
    "df = df \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .withColumn('dropoff_date', F.to_date(df.dropoff_datetime)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ae9bcb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "367170"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count number of trips that started on February 15\n",
    "df \\\n",
    "    .select('pickup_date') \\\n",
    "    .filter(df.pickup_date == '2021-02-15') \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ad2e51",
   "metadata": {},
   "source": [
    "### Using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6438c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a temp table from the data frame\n",
    "df.registerTempTable('table_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "981208d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:================================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  367170|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# count number of trips that started on February 15\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM table_df\n",
    "WHERE pickup_date = '2021-02-15'\n",
    ";\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70738e54",
   "metadata": {},
   "source": [
    "## Question 4. Longest trip for each day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc7fd8d",
   "metadata": {},
   "source": [
    "### Using pyspark data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9f3cd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the duration for each trip\n",
    "df = df \\\n",
    "    .withColumn(\"trip_duration\", (F.unix_timestamp(\"dropoff_datetime\") - F.unix_timestamp(\"pickup_datetime\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "90c24abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+-----------+------------+-------------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|pickup_date|dropoff_date|trip_duration|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+-----------+------------+-------------+\n",
      "|           HV0003|              B02764|2021-02-01 00:10:40|2021-02-01 00:21:09|          35|          39|   null| 2021-02-01|  2021-02-01|          629|\n",
      "|           HV0003|              B02764|2021-02-01 00:27:23|2021-02-01 00:44:01|          39|          35|   null| 2021-02-01|  2021-02-01|          998|\n",
      "|           HV0005|              B02510|2021-02-01 00:28:38|2021-02-01 00:38:27|          39|          91|   null| 2021-02-01|  2021-02-01|          589|\n",
      "|           HV0005|              B02510|2021-02-01 00:43:37|2021-02-01 01:23:20|          91|         228|   null| 2021-02-01|  2021-02-01|         2383|\n",
      "|           HV0003|              B02872|2021-02-01 00:08:42|2021-02-01 00:17:57|         126|         250|   null| 2021-02-01|  2021-02-01|          555|\n",
      "|           HV0003|              B02872|2021-02-01 00:26:02|2021-02-01 00:42:51|         208|         243|   null| 2021-02-01|  2021-02-01|         1009|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+-----------+------------+-------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa5d46fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:========================================>              (147 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|pickup_date|max(trip_duration)|\n",
      "+-----------+------------------+\n",
      "| 2021-02-11|             75540|\n",
      "| 2021-02-17|             57221|\n",
      "| 2021-02-20|             44039|\n",
      "| 2021-02-03|             40653|\n",
      "| 2021-02-19|             37577|\n",
      "| 2021-02-25|             35010|\n",
      "| 2021-02-18|             34612|\n",
      "| 2021-02-10|             34169|\n",
      "| 2021-02-21|             32223|\n",
      "| 2021-02-09|             32087|\n",
      "| 2021-02-06|             31447|\n",
      "| 2021-02-02|             30913|\n",
      "| 2021-02-05|             30511|\n",
      "| 2021-02-12|             30148|\n",
      "| 2021-02-08|             30106|\n",
      "| 2021-02-14|             29777|\n",
      "| 2021-02-22|             28278|\n",
      "| 2021-02-27|             27170|\n",
      "| 2021-02-15|             25874|\n",
      "| 2021-02-04|             25592|\n",
      "| 2021-02-16|             25441|\n",
      "| 2021-02-23|             24439|\n",
      "| 2021-02-26|             24422|\n",
      "| 2021-02-24|             23669|\n",
      "| 2021-02-13|             21442|\n",
      "| 2021-02-01|             20638|\n",
      "| 2021-02-28|             19850|\n",
      "| 2021-02-07|             17672|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:======================================================>(197 + 3) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# trip starting on which day was the longest?\n",
    "df \\\n",
    "    .groupBy('pickup_date') \\\n",
    "    .max('trip_duration') \\\n",
    "    .orderBy(F.col('max(trip_duration)').desc()) \\\n",
    "    .show(31)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce5a435",
   "metadata": {},
   "source": [
    "### Using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ce081373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the duration for each trip\n",
    "df_q42 = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    pickup_date\n",
    "    , MAX((unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime))) AS trip_length\n",
    "FROM table_df\n",
    "GROUP BY 1\n",
    "ORDER BY trip_length DESC\n",
    ";\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "264d8110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pickup_date: date (nullable = true)\n",
      " |-- trip_length: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_q42.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8975e387",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:===================================================>    (22 + 2) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|pickup_date|trip_length|\n",
      "+-----------+-----------+\n",
      "| 2021-02-11|      75540|\n",
      "| 2021-02-17|      57221|\n",
      "| 2021-02-20|      44039|\n",
      "| 2021-02-03|      40653|\n",
      "| 2021-02-19|      37577|\n",
      "| 2021-02-25|      35010|\n",
      "| 2021-02-18|      34612|\n",
      "| 2021-02-10|      34169|\n",
      "| 2021-02-21|      32223|\n",
      "| 2021-02-09|      32087|\n",
      "| 2021-02-06|      31447|\n",
      "| 2021-02-02|      30913|\n",
      "| 2021-02-05|      30511|\n",
      "| 2021-02-12|      30148|\n",
      "| 2021-02-08|      30106|\n",
      "| 2021-02-14|      29777|\n",
      "| 2021-02-22|      28278|\n",
      "| 2021-02-27|      27170|\n",
      "| 2021-02-15|      25874|\n",
      "| 2021-02-04|      25592|\n",
      "| 2021-02-16|      25441|\n",
      "| 2021-02-23|      24439|\n",
      "| 2021-02-26|      24422|\n",
      "| 2021-02-24|      23669|\n",
      "| 2021-02-13|      21442|\n",
      "| 2021-02-01|      20638|\n",
      "| 2021-02-28|      19850|\n",
      "| 2021-02-07|      17672|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_q42.show(31)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e171bc5",
   "metadata": {},
   "source": [
    "## Question 5. Most frequent dispatching_base_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c561dc7d",
   "metadata": {},
   "source": [
    "### Using pyspark data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "600f3994",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:=====================================================>  (23 + 1) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|dispatching_base_num|  count|\n",
      "+--------------------+-------+\n",
      "|              B02510|3233664|\n",
      "|              B02764| 965568|\n",
      "|              B02872| 882689|\n",
      "|              B02875| 685390|\n",
      "|              B02765| 559768|\n",
      "|              B02869| 429720|\n",
      "|              B02887| 322331|\n",
      "|              B02871| 312364|\n",
      "|              B02864| 311603|\n",
      "|              B02866| 311089|\n",
      "|              B02878| 305185|\n",
      "|              B02682| 303255|\n",
      "|              B02617| 274510|\n",
      "|              B02883| 251617|\n",
      "|              B02884| 244963|\n",
      "|              B02882| 232173|\n",
      "|              B02876| 215693|\n",
      "|              B02879| 210137|\n",
      "|              B02867| 200530|\n",
      "|              B02877| 198938|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .groupBy('dispatching_base_num') \\\n",
    "    .count() \\\n",
    "    .orderBy(F.col('count').desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949df748",
   "metadata": {},
   "source": [
    "### Using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4d35170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the most frequent dispatching_base_num  \n",
    "df_q5 = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    dispatching_base_num\n",
    "    , COUNT(dispatching_base_num)\n",
    "FROM table_df\n",
    "GROUP BY 1\n",
    "ORDER BY 2 DESC\n",
    ";\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a36b9186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- count(dispatching_base_num): long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_q5.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41d6d2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=================================================>       (21 + 3) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------------+\n",
      "|dispatching_base_num|count(dispatching_base_num)|\n",
      "+--------------------+---------------------------+\n",
      "|              B02510|                    3233664|\n",
      "|              B02764|                     965568|\n",
      "|              B02872|                     882689|\n",
      "|              B02875|                     685390|\n",
      "|              B02765|                     559768|\n",
      "|              B02869|                     429720|\n",
      "|              B02887|                     322331|\n",
      "|              B02871|                     312364|\n",
      "|              B02864|                     311603|\n",
      "|              B02866|                     311089|\n",
      "|              B02878|                     305185|\n",
      "|              B02682|                     303255|\n",
      "|              B02617|                     274510|\n",
      "|              B02883|                     251617|\n",
      "|              B02884|                     244963|\n",
      "|              B02882|                     232173|\n",
      "|              B02876|                     215693|\n",
      "|              B02879|                     210137|\n",
      "|              B02867|                     200530|\n",
      "|              B02877|                     198938|\n",
      "|              B02835|                     189031|\n",
      "|              B02888|                     169167|\n",
      "|              B02889|                     138762|\n",
      "|              B02836|                     128978|\n",
      "|              B02880|                     115716|\n",
      "|              B02395|                     112433|\n",
      "|              B02870|                     101945|\n",
      "|              B02800|                      84277|\n",
      "|              B02865|                      76160|\n",
      "|              B02512|                      41043|\n",
      "|              B02844|                       3502|\n",
      "+--------------------+---------------------------+\n",
      "only showing top 31 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 8:==============================================>        (170 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_q5.show(31)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6ad79d",
   "metadata": {},
   "source": [
    "## Question 6. Most common locations pair  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e1e39e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read zones \n",
    "df_zones = spark.read.parquet('zones/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5d7d7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "|        20|        Bronx|             Belmont|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zones.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1eedbc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a temp table from df_zones\n",
    "df_zones.registerTempTable('table_zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf67541c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the most frequent dispatching_base_num\n",
    "df_q6 = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    table_df.PULocationID\n",
    "    , table_df.DOLocationID\n",
    "    , CONCAT(COALESCE(zpu.Zone, 'Unknown'), ' / ', COALESCE(zdo.Zone, 'Unknown')) AS most_common_locations \n",
    "    , COUNT(*)\n",
    "FROM table_df\n",
    "LEFT JOIN table_zones AS zpu ON table_df.PULocationID = zpu.LocationID\n",
    "LEFT JOIN table_zones AS zdo ON table_df.DOLocationID = zdo.LocationID\n",
    "GROUP BY 1, 2, 3\n",
    "ORDER BY 4 DESC\n",
    ";\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0325f1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:=====================================================>  (23 + 1) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+---------------------+--------+\n",
      "|PULocationID|DOLocationID|most_common_locations|count(1)|\n",
      "+------------+------------+---------------------+--------+\n",
      "|          76|          76| East New York / E...|   45041|\n",
      "|          26|          26| Borough Park / Bo...|   37329|\n",
      "|          39|          39|  Canarsie / Canarsie|   28026|\n",
      "|          61|          61| Crown Heights Nor...|   25976|\n",
      "|          14|          14| Bay Ridge / Bay R...|   17934|\n",
      "|           7|           7|    Astoria / Astoria|   14688|\n",
      "|         129|         129| Jackson Heights /...|   14688|\n",
      "|          42|          42| Central Harlem No...|   14481|\n",
      "|          37|          37| Bushwick South / ...|   14424|\n",
      "|          89|          89| Flatbush/Ditmas P...|   13976|\n",
      "|         216|         216| South Ozone Park ...|   13716|\n",
      "|          35|          35| Brownsville / Bro...|   12829|\n",
      "|         132|         265|     JFK Airport / NA|   12542|\n",
      "|         188|          61| Prospect-Lefferts...|   11814|\n",
      "|          95|          95| Forest Hills / Fo...|   11548|\n",
      "|          36|          37| Bushwick North / ...|   11491|\n",
      "|          37|          36| Bushwick South / ...|   11487|\n",
      "|          61|         188| Crown Heights Nor...|   11462|\n",
      "|          61|         225| Crown Heights Nor...|   11342|\n",
      "|         188|         188| Prospect-Lefferts...|   11308|\n",
      "+------------+------------+---------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_q6.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
